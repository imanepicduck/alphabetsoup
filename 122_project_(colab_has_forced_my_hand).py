# -*- coding: utf-8 -*-
"""122 project (colab has forced my hand)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ujJ7HSdnUxORdYRpiBRURiGiz6jzaVkH
"""

import pandas as pd
import numpy as np
import cv2
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

from google.colab import files
data = files.upload()
X = np.load('image.npz')['arr_0']

from google.colab import files
data = files.upload()
y = pd.read_csv("labels.csv")["labels"]
print(pd.Series(y).value_counts())
classes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']
nclasses = len(classes)

samples_per_class = 5
figure = plt.figure(figsize=(nclasses*2, (1+samples_per_class*2)))

idx_cls = 0
for cls in classes:
  idxs = np.flatnonzero (y == cls)
  idxs = np.random.choice (idxs, samples_per_class, replace = False)
  i = 0
  for idx in idxs:
    plt_idx = i*nclasses + idx_cls + 1
    p = plt.subplot(samples_per_class, nclasses, plt_idx)
    p = sns.heatmap(np.reshape(X[idx], (22, 30), cmap=plt.cm.gray, xticklabels = False, yticklabels = False, cbar = False));
    p = plt.axis('off');
    i+=1
  idx_cls+=1

print(len(X))
print(len(y))

print(X[0])
print(y[0])

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=9, train_size=7500, test_size=2500)
X_trained_scaled = X_train/255.0
X_tested_scaled = X_test/255.0

clf = LogisticRegression(solver='saga', multi_class='multinomial').fit(X_trained_scaled, y_train)

y_pred = clf.predict(X_tested_scaled)
accuracy=accuracy_score(y_test, y_pred)
print(accuracy)

cm = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])
p = plt.figure(figsize=(10,10));
p = sns.heatmap(cm, annot=True, fmt = 'd', cbar = False)